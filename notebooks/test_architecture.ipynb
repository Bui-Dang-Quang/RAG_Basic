{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff597c1",
   "metadata": {},
   "source": [
    "# <b>Ingest Dev Sever Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf82af6",
   "metadata": {},
   "source": [
    "https://medium.com/@ajayverma23/the-art-and-science-of-rag-mastering-prompt-templates-and-contextual-understanding-a47961a57e27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5588afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG\\requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec434f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG\\src\\main.py\"\n",
    "# import logging\n",
    "# from fastapi import FastAPI\n",
    "# import inngest\n",
    "# import inngest.fast_api\n",
    "# from inngest.experimental import ai \n",
    "# from dotenv import load_dotenv\n",
    "# import uvicorn\n",
    "# import uuid\n",
    "# import os\n",
    "# import datetime\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# inngest_client = inngest.Inngest(\n",
    "#     app_id = \"rag_remake_app\",\n",
    "#     logger = logging.getLogger(\"uvicorn\"), ## Possible values: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "#     is_production = False,\n",
    "#     serializer = inngest.PydanticSerializer()\n",
    "# )\n",
    "\n",
    "\n",
    "# @inngest_client.create_function(\n",
    "#     fn_id = \"RAG: Ingest PDF Document\",\n",
    "#     trigger=inngest.TriggerEvent(event=\"rag/pdf_document/ingest\")\n",
    "# )\n",
    "\n",
    "# async def ingest_pdf_document(ctx: inngest.Context):\n",
    "#     return {'hello': 'world'}\n",
    "\n",
    "# app = FastAPI()\n",
    "# inngest.fast_api.serve(app, inngest_client, [ingest_pdf_document])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e88ac7",
   "metadata": {},
   "source": [
    "# <b> Vector Store Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c815aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\qdrant_database.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\qdrant_database.py\"\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "class QdrantStore:\n",
    "    def __init__(self, url: str = \"http://localhost:6333\", collection_name: str =\"PDFdoc\", dim=1024) -> None:\n",
    "        self.client: QdrantClient = QdrantClient(url=url)\n",
    "        self.collection_name: str = collection_name\n",
    "        \n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=dim, \n",
    "                    distance=Distance.COSINE\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "    def count_vectors(self) -> int:\n",
    "            \"\"\"Returns the number of vectors currently in the collection\"\"\"\n",
    "            return self.client.count(\n",
    "                collection_name=self.collection_name\n",
    "            ).count\n",
    "    \n",
    "    def upsert(self, ids, vectors, payloads):\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=ids[i], \n",
    "                vector=vectors[i], \n",
    "                payload=payloads[i]\n",
    "            )\n",
    "            for i in range(len(ids))\n",
    "        ]\n",
    "\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "    def similarity_search(self, query_vector, top_k: int = 5):\n",
    "        if hasattr(query_vector, 'tolist'):\n",
    "            query_vector = query_vector.tolist()\n",
    "\n",
    "        # 2. Use 'query_points' (Robust replacement for .search)\n",
    "        results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_vector,\n",
    "            limit=top_k,\n",
    "        ).points\n",
    "\n",
    "\n",
    "        contexts = []\n",
    "        sources = set()\n",
    "\n",
    "        for r in results:\n",
    "            payload = r.payload or {}\n",
    "            text = payload.get('text', '')\n",
    "            source = payload.get('source', '')\n",
    "            if text:\n",
    "                contexts.append(text)\n",
    "                sources.add(source)\n",
    "\n",
    "        return {\n",
    "            'contexts': contexts,\n",
    "            'sources': list(sources)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c257325",
   "metadata": {},
   "source": [
    "# <b> PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56614c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\pdf_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\pdf_loader.py\"\n",
    "from llama_parse import LlamaParse  \n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llamaparse_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "\n",
    "class PDFLoader:\n",
    "    def __init__(self, file_path:str):\n",
    "        self.file_path = file_path\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        self.cache_path = os.path.join(\"cache\", f\"{file_name}.pkl\")\n",
    "        self.markdown_path = os.path.join(\"markdown\", f\"{file_name}.md\")\n",
    "    \n",
    "        os.makedirs(\"cache\", exist_ok=True)\n",
    "        os.makedirs(\"markdown\", exist_ok=True)\n",
    "\n",
    "    def load_docs(self):\n",
    "\n",
    "        # Check if the parsed data file exists\n",
    "        if os.path.exists(self.cache_path):\n",
    "            # Load the parsed data from the file\n",
    "            with open(self.cache_path, \"rb\") as f:\n",
    "                parsed_data = pickle.load(f)\n",
    "            print(f\"Loaded parsed data from cache: {self.cache_path}\")\n",
    "            return parsed_data\n",
    "        else:\n",
    "            # Parse PDF and extract text\n",
    "            parser = LlamaParse(\n",
    "                api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "                result_type = \"markdown\",\n",
    "            )\n",
    "\n",
    "            file_extractor = {\".pdf\": parser}\n",
    "\n",
    "            documents = SimpleDirectoryReader(\n",
    "                input_files=[self.file_path], \n",
    "                file_extractor=file_extractor\n",
    "            ).load_data()\n",
    "\n",
    "            # Save the parsed data to a file for future use\n",
    "            print(f\"Saving parsed data to cache: {self.cache_path}\")\n",
    "            with open(self.cache_path, \"wb\") as f:\n",
    "                pickle.dump(documents, f)\n",
    "            \n",
    "            # Save the raw text content to a Markdown file\n",
    "            print(f\"Saving markdown content to: {self.markdown_path}\")\n",
    "            with open(self.markdown_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                # LlamaParse might return multiple document parts, we join them\n",
    "                for doc in documents:\n",
    "                    f.write(doc.text + \"\\n\\n\")\n",
    "            return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b565394",
   "metadata": {},
   "source": [
    "# <b> Document Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4612b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\document_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\document_processor.py\"\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, model_name: str = \"BAAI/bge-m3\"):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    \n",
    "    def chunk_document(self, text: str) -> list[str]:\n",
    "        \"\"\"Chunk a document into smaller pieces based on token count\"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        total_tokens = len(tokens)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < total_tokens:\n",
    "            end = min(start + self.chunk_size, total_tokens)\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def preprocess_documents(self, documents: list[str]) -> list[str]:\n",
    "        \"\"\"Preprocess a list of documents by chunking them\"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.chunk_document(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "        return all_chunks\n",
    "    \n",
    "    def chunk_Recursive_char(self, texts: str) -> list[str]:\n",
    "        \"\"\"Chunk a document using RecursiveCharacterTextSplitter\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap, # Use the value from init\n",
    "            length_function=self.count_tokens,\n",
    "            separators=self.separators, # Standard recursive separators\n",
    "            keep_separator=False,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "        # Handle Input Flexibility (String vs List)\n",
    "        if isinstance(texts, str):\n",
    "            input_list = [texts] # Wrap single string in list\n",
    "        else:\n",
    "            input_list = texts\n",
    "\n",
    "        # 3. Process Statelessly \n",
    "        final_chunks = []\n",
    "        \n",
    "        for text in input_list:\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            final_chunks.extend(chunks)\n",
    "            \n",
    "        return final_chunks\n",
    "\n",
    "    def chunk_by_markdown(self, markdown_txt: str):\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Title\"),\n",
    "            (\"##\", \"Section\"),\n",
    "            (\"###\", \"Subsection\"),\n",
    "        ]\n",
    "        \n",
    "        # 2. Split\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_txt)\n",
    "\n",
    "        return md_header_splits\n",
    "\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Helper to count tokens in a string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "    # \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Load and Chunk Documents\n",
    "    # ---------------------------------------------------------\n",
    "    # \"\"\"\n",
    "    # loader = PDFLoader(\"C:\\\\Users\\\\dangq\\\\OneDrive\\\\M√°y t√≠nh\\\\USTH\\\\ICT\\\\Internship\\\\RAG Remake\\\\RAG\\\\data\\\\raw\\\\2405.17247v1.pdf\")\n",
    "    # docs = loader.load_docs()\n",
    "    # # print(docs[10].text)\n",
    "\n",
    "    # docs_texts = [doc.text for doc in docs]\n",
    "    # processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
    "    # chunks = processor.chunk_Recursive_char(docs_texts)\n",
    "    # print(\"=====\"*50)\n",
    "    # print(f\"\\nCreated {len(chunks)} chunks.\")\n",
    "\n",
    "    # # Uncomment to see some sample chunks\n",
    "    # # for i, chunk in enumerate(chunks[5:10]):  # Print first 5 chunks\n",
    "    # #     print(f\"Chunk {i+1} (Length: {processor.count_tokens(chunk)} tokens): {chunk}\\n\")\n",
    "    \n",
    "\n",
    "    # \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Generate Embeddings (The missing step)\n",
    "    # ---------------------------------------------------------\n",
    "    # \"\"\"\n",
    "    # # embed_model = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "    # embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "    # vectors = embed_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Prepare Data for Qdrant\n",
    "    # ---------------------------------------------------------\n",
    "    # \"\"\"\n",
    "    # ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "    # payloads = [\n",
    "    #     {\"text\": chunk, \"source\": \"2405.17247v1.pdf\", \"chunk_index\": i} \n",
    "    #     for i, chunk in enumerate(chunks)\n",
    "    # ]\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Upsert to Vector Database\n",
    "    # ---------------------------------------------------------\n",
    "    # \"\"\"\n",
    "    # vector_store = QdrantStore(dim=1024, collection_name=\"PDFdoc_test\")\n",
    "    # vector_store.upsert(ids, vectors, payloads)\n",
    "    # print(\"Upsert completed.\")\n",
    "\n",
    "    # \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Test Search Functionality\n",
    "    # ---------------------------------------------------------\n",
    "    # \"\"\"\n",
    "    # print(\"=====\"*50)\n",
    "    # print(\"Testing search functionality...\")\n",
    "    # query = \"What is the core idea of Contrastive-based VLMs?\"\n",
    "\n",
    "    # # We must embed the query using the SAME model\n",
    "    # query_vector = embed_model.encode(query).tolist()\n",
    "\n",
    "    # search_results = vector_store.similarity_search(query_vector, top_k=3)\n",
    "    # print(f\"Query: {query}\\n\")\n",
    "    # for context in search_results['contexts']:\n",
    "    #     print(f\"--- Result ---\\n{context[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b683c7",
   "metadata": {},
   "source": [
    "# <b> LLM Wrapper Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad912d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the content\n",
    "# content = \"\"\"\n",
    "# ===================================================================\n",
    "# RAG EVALUATION: HARD QUESTIONS (STATISTICAL & ARCHITECTURAL SPECS)\n",
    "# Source Document: 2405.17247v1.pdf (An Introduction to VLM)\n",
    "# ===================================================================\n",
    "\n",
    "# [Test Case 1: Zero-Shot Performance]\n",
    "# Question: What was the specific zero-shot classification accuracy attained by the ResNet-101 CLIP model, and which supervised model did it match?\n",
    "# Answer: The ResNet-101 CLIP model attained 76.2% zero-shot classification accuracy, matching the performance of a supervised ResNet model.\n",
    "\n",
    "# [Test Case 2: Training Compute Resources]\n",
    "# Question: How many GPUs and how much time were required for the first round of training for MiniGPT-4?\n",
    "# Answer: The first round of training for MiniGPT-4 required only four A100 GPUs for around ten hours.\n",
    "\n",
    "# [Test Case 3: Technical Specifications - Tokenizer]\n",
    "# Question: What are the specific tokenization parameters (image size, token count, and vocabulary size) used by the CM3Leon image tokenizer borrowed from Gafni et al.?\n",
    "# Answer: It encodes a 256x256 image into 1024 tokens from a vocabulary of 8192.\n",
    "\n",
    "# [Test Case 4: Fine-Tuning Efficiency]\n",
    "# Question: When using the weight-sharing technique with VL-adapter, what specific percentage of total parameters requires updating for video-text tasks?\n",
    "# Answer: For video-text tasks, only 3.39% of the total parameters require updating.\n",
    "\n",
    "# [Test Case 5: Benchmark Improvement]\n",
    "# Question: On the MMHAL-BENCH, by what percentage does LLAVA-RLHF outperform baselines, and what is the specific focus of this benchmark?\n",
    "# Answer: LLAVA-RLHF outperforms baselines by 60% on MMHAL-BENCH, which has a special focus on penalizing hallucinations.\n",
    "# \"\"\"\n",
    "\n",
    "# # Save to file\n",
    "# file_path = \"C:\\\\Users\\\\dangq\\\\OneDrive\\\\M√°y t√≠nh\\\\USTH\\\\ICT\\\\Internship\\\\RAG Remake\\\\RAG\\\\data\\\\raw\\\\rag_hard_questions.txt\"\n",
    "# with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(content.strip())\n",
    "\n",
    "# print(f\"File successfully created at: {os.path.abspath(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a33061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lmstudio as lms\n",
    "\n",
    "# # model = lms.llm(\"meta-llama-3.1-8b-instruct\")\n",
    "\n",
    "# client = lms.Client(api_host=\"127.0.0.1:1234\")\n",
    "# model = client.llm.model(\"microsoft_-_phi-3.5-mini-instruct\")\n",
    "# result = model.respond(\"Hello\")\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1306ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\llm\\custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\llm\\custom_model.py\"\n",
    "import lmstudio as lms\n",
    "class CustomModel:\n",
    "    def __init__(self, model_name: str = \"microsoft_-_phi-3.5-mini-instruct\", api_host: str = \"127.0.0.1:1234\"):\n",
    "        self.model_name = model_name\n",
    "        self.api_host = api_host\n",
    "        self.client = None\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Connecting to: {self.model_name}... \")\n",
    "            self.client = lms.Client(api_host=self.api_host)\n",
    "            self.model = self.client.llm.model(self.model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Connection Error: {e}\")\n",
    "            print(\"Tip: Ensure LM Studio is open and 'Start Server' is green.\")\n",
    "        \n",
    "    def generate(self, user_query: str, context_input) -> str:\n",
    "\n",
    "        if not self.model:\n",
    "            return \"Error: Model not loaded.\"\n",
    "        \n",
    "        if isinstance(context_input, list):\n",
    "            context_str = \"\\n\\n\".join(context_input)\n",
    "        else:\n",
    "            context_str = str(context_input)\n",
    "        \n",
    "        system_instruction = f\"\"\"\n",
    "            Answer the question below by first outlining the main points of context relevant to the question,\n",
    "            then use that outline to generate the final answer. \n",
    "\n",
    "            Context: \n",
    "            {context_str}\n",
    "\n",
    "            Question: \n",
    "            {user_query}\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"System Instruction: \\n\")\n",
    "        print(system_instruction)\n",
    "        print(\"===\"*50)\n",
    "        \n",
    "        try: \n",
    "            print(\"===\"*50)\n",
    "            response = self.model.respond(system_instruction)\n",
    "            return \"Generative Answer: \\n\" + str(response)\n",
    "        except Exception as e:\n",
    "            return f\"Generate Error: {e}\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "#     vector_store = QdrantStore(dim=1024, collection_name=\"PDFdoc_test\")\n",
    "#     query = \"What is the core idea of Contrastive-based VLMs?\"\n",
    "\n",
    "#     # We must embed the query using the SAME model\n",
    "#     query_vector = embed_model.encode(query).tolist()\n",
    "\n",
    "#     search_results = vector_store.similarity_search(query_vector, top_k=3)\n",
    "#     # print(f\"Query: {query}\\n\")\n",
    "#     # for context in search_results['contexts']:\n",
    "#     #     print(f\"--- Result ---\\n{context[:200]}...\\n\")\n",
    "\n",
    "#     llm = CustomModel()\n",
    "#     answer = llm.generate(query, search_results['contexts'])\n",
    "#     print(answer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50775dbc",
   "metadata": {},
   "source": [
    "# <b> MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be71cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\examples\\chat_session.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\examples\\chat_session.py\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (which is 'RAG_Basic')\n",
    "project_root = os.path.dirname(current_dir)\n",
    "print(project_root)\n",
    "\n",
    "# Add the project root to the system path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from src.llm import CustomModel\n",
    "from src.utils import DocumentProcessor, PDFLoader, QdrantStore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "\"\"\"\n",
    "---------------------------------------------------------\n",
    "1. SET UP\n",
    "---------------------------------------------------------\n",
    "\"\"\"\n",
    "llm = CustomModel()\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
    "vector_store = QdrantStore(dim=1024, collection_name=\"TEST\")\n",
    "loader = PDFLoader(\"C:\\\\Users\\\\dangq\\\\OneDrive\\\\M√°y t√≠nh\\\\USTH\\\\ICT\\\\Internship\\\\RAG Remake\\\\RAG\\\\data\\\\raw\\\\2405.17247v1.pdf\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "---------------------------------------------------------\n",
    "2. Read and Chunk Documents (Preproces):\n",
    "    - Chunking\n",
    "    - Create vectors embedding for each chunks\n",
    "    - Construct Metadata (need to improve in the future)\n",
    "    - Upsert the data to vector database\n",
    "---------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Check if we already have data\n",
    "current_count = vector_store.count_vectors()\n",
    "\n",
    "if current_count == 0:\n",
    "    print(\"===\"*50)\n",
    "    print(\"Collection is empty. Starting processing and ingestion...\")\n",
    "    \n",
    "    docs = loader.load_docs()\n",
    "    docs_texts = [doc.text for doc in docs]\n",
    "\n",
    "    chunks = processor.chunk_Recursive_char(docs_texts)\n",
    "    print(f\"\\nCreated {len(chunks)} chunks.\")\n",
    "\n",
    "    # Generate Embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    vectors = embed_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "    # Prepare Metadata for Qdrant\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "    payloads = [\n",
    "        {\"text\": chunk, \"source\": \"2405.17247v1.pdf\", \"chunk_index\": i} \n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Upsert into vector database\n",
    "    vector_store.upsert(ids, vectors, payloads)\n",
    "    print(\"Upsert completed. Your Collection is ready to use!\")\n",
    "    print(\"===\"*50)\n",
    "    \n",
    "else:\n",
    "    print(\"===\"*50)\n",
    "    print(f\"Collection '{vector_store.collection_name}' already contains {current_count} vectors.\")\n",
    "    print(\"Skipping Step 2 (Ingestion).\")\n",
    "    print(\"===\"*50)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "---------------------------------------------------------\n",
    "3. Test Similarity Search \n",
    "---------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# We must embed the query using the SAME model\n",
    "\n",
    "query = \" On the MMHAL-BENCH, by what percentage does LLAVA-RLHF outperform baselines, and what is the specific focus of this benchmark?\"\n",
    "query_vector = embed_model.encode(query).tolist()\n",
    "search_results = vector_store.similarity_search(query_vector, top_k=3)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "---------------------------------------------------------\n",
    "4. Run Inference\n",
    "---------------------------------------------------------\n",
    "\"\"\"\n",
    "print(\"===\"*100)\n",
    "answer = llm.generate(query, search_results['contexts'])\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b24e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # 1. Define your input and output filenames\n",
    "# input_pkl_path = r\"C:\\\\Users\\\\dangq\\\\OneDrive\\\\M√°y t√≠nh\\\\USTH\\\\ICT\\\\Internship\\\\RAG Remake\\\\RAG\\\\data\\\\raw\\\\2405.17247v1.pdf.pkl\"\n",
    "# output_md_path = r\"C:\\\\Users\\\\dangq\\\\OneDrive\\\\M√°y t√≠nh\\\\USTH\\\\ICT\\\\Internship\\\\RAG Remake\\\\RAG\\\\data\\\\raw\\\\2405.17247v1.md\"  # We save as .md to keep formatting\n",
    "\n",
    "# def convert_pickle_to_markdown():\n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(input_pkl_path):\n",
    "#         print(f\"‚ùå Error: File not found at {input_pkl_path}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"üìñ Loading {input_pkl_path}...\")\n",
    "    \n",
    "#     # 2. Load the data from pickle\n",
    "#     with open(input_pkl_path, \"rb\") as f:\n",
    "#         documents = pickle.load(f)\n",
    "    \n",
    "#     # Verify it's what we expect (List of Documents)\n",
    "#     print(f\"‚úÖ Loaded {len(documents)} document chunk(s).\")\n",
    "    \n",
    "#     # 3. Extract and combine text\n",
    "#     # LlamaParse often returns one large document, or split pages. \n",
    "#     # We join them just in case.\n",
    "#     full_markdown_text = \"\"\n",
    "    \n",
    "#     for i, doc in enumerate(documents):\n",
    "#         # The '.text' attribute contains the parsed markdown\n",
    "#         content = doc.text\n",
    "        \n",
    "#         # Optional: Add a separator if you have multiple chunks\n",
    "#         if i > 0:\n",
    "#             full_markdown_text += \"\\n\\n--- SEGMENT BREAK ---\\n\\n\"\n",
    "            \n",
    "#         full_markdown_text += content\n",
    "\n",
    "#     # 4. Save to a new file\n",
    "#     with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(full_markdown_text)\n",
    "        \n",
    "#     print(f\"üéâ Success! Converted text saved to: {output_md_path}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     convert_pickle_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a1d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\llm\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\llm\\__init__.py\"\n",
    "from .custom_model import CustomModel\n",
    "\n",
    "__all__ = [\"CustomModel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d281e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"C:\\Users\\dangq\\OneDrive\\M√°y t√≠nh\\USTH\\ICT\\Internship\\RAG Remake\\RAG_Basic\\src\\utils\\__init__.py\"\n",
    "from .document_processor import DocumentProcessor\n",
    "from .pdf_loader import PDFLoader\n",
    "from .qdrant_database import QdrantStore\n",
    "\n",
    "__all__ = [\"DocumentProcessor\", \"PDFLoader\", \"QdrantStore\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_remake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
